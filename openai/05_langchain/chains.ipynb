{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08743e84-8184-49ab-82b1-e04786b330ba",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "## Demo code to use LangChain\n",
    "A quick demo code to use chains in LangChain.\n",
    "\n",
    "Please ensure you have *.env* file in your HOME/Documents/src/openai/ folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8cecc-66e9-4800-bd2d-f9ee81766027",
   "metadata": {},
   "source": [
    "## Initialize the model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1885fd-a3cb-4d4c-ac31-f70127c5f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b37a6-80e4-479f-b155-c586779e590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "env_path = os.getenv(\"HOME\") + \"/Documents/src/openai/.env\"\n",
    "load_dotenv(dotenv_path=env_path, verbose=True)\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://pvg-azure-openai-uk-south.openai.azure.com\"\n",
    "\n",
    "model = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62bd4fc-0e95-4c94-976c-3c32bdb1d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\"type\": \"function\", \n",
    "         \"function\": {\n",
    "            \"name\": \"joke\", \n",
    "            \"description\": \"Generate a joke\",\n",
    "            \"parameters\": {\n",
    "                \"type\":\"object\",\n",
    "                \"properties\": {\n",
    "                    \"setup\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"description\": \"the setup for the joke\"\n",
    "                    },\n",
    "                    \"punchline\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"description\": \"the punchline for the joke\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"setup\", \"punchline\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b96609-438d-4f15-ae36-df8582019c30",
   "metadata": {},
   "source": [
    "## Simple LCEL\n",
    "\n",
    "1. ChatOpenAI to invoke prompt directly\n",
    "2. Simplest LCEL\n",
    "3. Inject simple command into ChatOpenAI\n",
    "4. Simple LCEL with standard format: prompt | model | output_parser\n",
    "5. Inject tools call into ChatOpenAI\n",
    "6. Tools call with ToolsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe4a67-3182-4797-a175-c9089a36be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('tell me a joke about {topic}')\n",
    "\n",
    "response = model.invoke(prompt.format(topic='programmers'))\n",
    "print(\"1. Direct model output:\")\n",
    "pprint(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "chain_origin = prompt | model\n",
    "response = chain_origin.invoke({'topic':'programmers'})\n",
    "print(\"2. Simple chain output:\")\n",
    "pprint(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "chain_with_stop_bind = prompt | model.bind(stop=['?'])\n",
    "response = chain_with_stop_bind.invoke({'topic':'programmers'})\n",
    "print(\"3. Chain with stop bind output:\")\n",
    "pprint(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "chain_with_parser = prompt | model | StrOutputParser()\n",
    "response = chain_with_parser.invoke({'topic':'programmers'})\n",
    "print(\"4. Chain with String Output Parser output:\")\n",
    "pprint(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "chain_with_tools_bind = prompt | model.bind(tools=tools, tool_choice={'type': 'function', 'function': {'name': 'joke'}})\n",
    "response = chain_with_tools_bind.invoke({'topic':'programmers'}, config={})\n",
    "print(\"5. Chain with Tools Call output:\")\n",
    "pprint(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "chain_with_tools_bind = prompt | model.bind(tools=tools) | JsonOutputToolsParser()\n",
    "response = chain_with_tools_bind.invoke({'topic':'programmers'}, config={})\n",
    "print(\"6. Chain with Tools Parser output:\")\n",
    "pprint(response)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2a349-511b-4f52-9366-91f99c02332d",
   "metadata": {},
   "source": [
    "## Simple Chain\n",
    "\n",
    "1. Test 1st prompt\n",
    "2. Test 2nd prompt with hard code input\n",
    "3. Combine 2 prompts together\n",
    "4. SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d03f1-16fe-48a7-a1d4-85dfee95bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain, SequentialChain\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the year {person} won the gold medal in Olmypics?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"who's the chairman of China in the year of {year}? respond in {language}\")\n",
    "\n",
    "print(\"1. Chain1 output:\")\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "response = chain1.invoke({\"person\": \"刘翔\"})\n",
    "pprint(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"2. Chain2 output:\")\n",
    "chain2 = (\n",
    "    {\"year\": itemgetter(\"year\"), \"language\": itemgetter(\"language\")}\n",
    "     | prompt2 | model | StrOutputParser()\n",
    ")\n",
    "response = chain2.invoke({\"year\": 2004, \"language\": \"Chinese\"})\n",
    "pprint(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"3. LCEL Chain output:\")\n",
    "chain3 = (\n",
    "    {\"year\": chain1, \"language\": itemgetter(\"language\")}\n",
    "     | prompt2 | model | StrOutputParser()\n",
    ")\n",
    "response = chain3.invoke({\"person\": \"刘翔\", \"language\": \"Chinese\"})\n",
    "pprint(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"4. SequentialChain output:\")\n",
    "subChain1 = LLMChain(llm=model, prompt=prompt1, output_key=\"year\")\n",
    "subChain2 = LLMChain(llm=model, prompt=prompt2, output_key=\"chairman\")\n",
    "chain4 = SequentialChain(chains=[subChain1, subChain2], input_variables=[\"person\", \"language\"], verbose=False)\n",
    "response = chain4.invoke({\"person\": \"刘翔\", \"language\": \"Chinese\"})\n",
    "pprint(response['chairman'])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"5. SimpleSequentialChain output:\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"who's the chairman of China in the year of {year}?\")\n",
    "subChain1 = LLMChain(llm=model, prompt=prompt1)\n",
    "subChain2 = LLMChain(llm=model, prompt=prompt2)\n",
    "chain5 = SimpleSequentialChain(chains=[subChain1, subChain2], verbose=False)\n",
    "response = chain5.invoke(\"刘翔\")\n",
    "pprint(response['output'])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b603b5-631d-4090-a3a6-ab61bfc98802",
   "metadata": {},
   "source": [
    "## A More Complex Chain\n",
    "\n",
    "1. input: prompt; output: year\n",
    "2. input: year; output: USA president's name 1\n",
    "3. input: year; output: USA president's name 2\n",
    "4. input: USA president's name 1, USA president's name 2; output: who's popular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951e4ce-bf50-46ff-8968-112c6b39ef18",
   "metadata": {},
   "source": [
    "<img src=\"images/langchain-chains.jpg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e3cac-542e-4c86-9788-5aaedc462d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableMap\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what's year when 刘翔 won the Olympics gold medal? Please just give the year.\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Who held the position of president of the United States 5 years prior to {year}? Please just give the name.\")\n",
    "prompt3 = ChatPromptTemplate.from_template(\"Who was the president of the United States 5 years following {year}? Please just give the name.\")\n",
    "prompt4 = ChatPromptTemplate.from_template(\"who's more popular in USA? {president1} or {president2}?\")\n",
    "\n",
    "print(\"1. Chain1 output:\")\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "response = chain1.invoke({})\n",
    "pprint(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"2. Chain2 output:\")\n",
    "chain2 = {\"year\":itemgetter(\"year\")} | {\n",
    "    \"president1\": prompt2 | model | StrOutputParser(),\n",
    "    \"president2\": prompt3 | model | StrOutputParser()\n",
    "} | prompt4 | model | StrOutputParser()\n",
    "#response = chain2.invoke({\"year\": 2004}, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "response = chain2.invoke({\"year\": 2004})\n",
    "pprint(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"3. Composition Chain output:\")\n",
    "chain_composition = RunnableMap({\"year\":chain1}) | {\n",
    "    \"president1\": prompt2 | model | StrOutputParser(),\n",
    "    \"president2\": prompt3 | model | StrOutputParser()\n",
    "} | prompt4 | model | StrOutputParser()\n",
    "#response = chain_composition.invoke({}, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "response = chain_composition.invoke({})\n",
    "pprint(response)\n",
    "chain_composition.get_graph().print_ascii()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"4. SequentialChain output:\")\n",
    "subChain1 = LLMChain(llm=model, prompt=prompt1, output_key=\"year\", verbose=False)\n",
    "subChain2 = LLMChain(llm=model, prompt=prompt2, output_key=\"president1\", verbose=False)\n",
    "subChain3 = LLMChain(llm=model, prompt=prompt3, output_key=\"president2\", verbose=False)\n",
    "subChain4 = LLMChain(llm=model, prompt=prompt4, output_key=\"president\", verbose=False)\n",
    "chain_sequence = SequentialChain(chains=[subChain1, \n",
    "                                         SequentialChain(\n",
    "                                             chains=[subChain2, subChain3], \n",
    "                                             input_variables=[\"year\"], \n",
    "                                             output_variables=[\"president1\", \"president2\"], \n",
    "                                             verbose=False\n",
    "                                            ), \n",
    "                                         subChain4], \n",
    "                                 input_variables=[], \n",
    "                                 verbose=False)\n",
    "response = chain_sequence.invoke({})\n",
    "pprint(response['president'])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73756693-4f3d-4bdb-9355-adaad865552e",
   "metadata": {},
   "source": [
    "## Chain with Json Formatted Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da6969-cb21-4f65-8a53-c4493e91140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class SortEnum(str, Enum):\n",
    "    data = 'data'\n",
    "    price = 'price'\n",
    "\n",
    "class OrderingEnum(str, Enum):\n",
    "    ascend = 'ascend'\n",
    "    descend = 'descend'\n",
    "\n",
    "class Semantics(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"Mobile Package Name\",default=None)\n",
    "    price_lower: Optional[int] = Field(description=\"Price Floor\",default=None)\n",
    "    price_upper: Optional[int] = Field(description=\"Price Cap\",default=None)\n",
    "    data_lower: Optional[int] = Field(description=\"Data Floor\",default=None)\n",
    "    data_upper: Optional[int] = Field(description=\"Data Cap\",default=None)\n",
    "    sort_by: Optional[SortEnum] = Field(description=\"Order by Price or Data\",default=None)\n",
    "    ordering: Optional[OrderingEnum] = Field(description=\"Ascend Order or Descend Order\",default=None)\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Semantics)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Parse user's input as JSON. The format as below：\\n{format_instructions}\\nDon't output anything which is not mentioned by user\",\n",
    "        ),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()} | prompt | model | parser\n",
    ")\n",
    "\n",
    "pprint(chain.invoke(\"不超过100元且不小于30元，且流量要超过50M的中国电信套餐有哪些\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65206afe-2dcb-48dc-9045-dafc42a667ba",
   "metadata": {},
   "source": [
    "## Chain with a VectorStore for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4436b76-7b96-4154-ba02-325f80a0452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub\n",
    "\n",
    "vectorstore = Chroma.from_texts(\n",
    "    [\n",
    "        \"Sam Altman is the CEO of OpenAI\", \n",
    "        \"Christian Klein is the CEO of SAP\",\n",
    "        \"PHP is the best programming language\",\n",
    "        \"Virtual Threads in recent Java versions send WebFlux back home\"\n",
    "    ], embedding=AzureOpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "qachain_legacy = RetrievalQA.from_chain_type(model, retriever=retriever)\n",
    "\n",
    "qachain_lcel = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "qachain_lcel2 = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": RunnablePassthrough() | retriever} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(model, retrieval_qa_chat_prompt)\n",
    "qachain_neo = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "question = \"SAP的CEO是谁\"\n",
    "\n",
    "print(\"1. RetrievalQA chain output:\")\n",
    "pprint(qachain_legacy.invoke(question)['result'])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"2. LCEL chain output:\")\n",
    "pprint(qachain_lcel.invoke(question))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"3. LCEL2 chain output:\")\n",
    "pprint(qachain_lcel2.invoke(question))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"4. create_retrieval_chain chain output:\")\n",
    "pprint(qachain_neo.invoke({\"input\":question})['answer'])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4636b1b-5707-436c-96a4-72fa6a7d1b29",
   "metadata": {},
   "source": [
    "## Router Chain Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8f93c-2c1a-4bc2-9baf-4bb540b7462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_tagging_chain_pydantic  \n",
    "from langchain.schema.runnable import RouterRunnable, RunnableLambda, RunnableBranch\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "  \n",
    "class PromptToUse(BaseModel):  \n",
    "    \"\"\"Used to determine which prompt to use to answer the user's input.\"\"\"  \n",
    "      \n",
    "    name: str = Field(description=\"Should be one of `math`, `chemistry`, `physics`\")  \n",
    "\n",
    "tagger = create_tagging_chain_pydantic(PromptToUse, model)\n",
    "\n",
    "math_template = \"You are a math genius. Answer the question: {input}\"\n",
    "physics_template = \"You are a physics professor. Answer the question: {input}\"\n",
    "chemistry_template = \"You are a chemistry expert. Answer the question: {input}\"\n",
    "\n",
    "chain1 = ChatPromptTemplate.from_template(math_template) | model\n",
    "chain2 = ChatPromptTemplate.from_template(physics_template) | model\n",
    "chain3 = ChatPromptTemplate.from_template(chemistry_template) | model\n",
    "\n",
    "#router runnable part\n",
    "router_runnable = RouterRunnable({\"math\": chain1, \"physics\": chain2, \"chemistry\": chain3})\n",
    "\n",
    "chain_runnable = {\n",
    "\"key\": {\"input\": lambda x: x[\"input\"]} | tagger | (lambda x: x['text'].name),\n",
    "\"input\": {\"input\": lambda x: x[\"input\"]}\n",
    "} | router_runnable\n",
    "\n",
    "#runnable lambda part\n",
    "def routeFunc(info):\n",
    "    if \"math\" in info[\"topic\"].lower():\n",
    "        return chain1\n",
    "    elif \"physics\" in info[\"topic\"].lower():\n",
    "        return chain2\n",
    "    else:\n",
    "        return chain3\n",
    "\n",
    "chain_judge = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user question below, classify it as either being about `math`, `chemistry`, or `physics`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<input>\n",
    "{input}\n",
    "</input>\n",
    "\n",
    "Classification:\"\"\"\n",
    "    )\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_lambda = {\n",
    "    \"topic\": chain_judge, \"input\": lambda x: x[\"input\"]\n",
    "} | RunnableLambda(routeFunc)\n",
    "\n",
    "#runnable branch part\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"math\" in x[\"topic\"].lower(), chain1),\n",
    "    (lambda x: \"physics\" in x[\"topic\"].lower(), chain2),\n",
    "    (lambda x: \"chemistry\" in x[\"topic\"].lower(), chain3),\n",
    "    chain3\n",
    ")\n",
    "chain_branch = {\"topic\": chain_judge, \"input\": lambda x: x[\"input\"]} | branch\n",
    "\n",
    "#legacy router chain part\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"math\",\n",
    "        \"description\": \"answer math puzzles\",\n",
    "        \"prompt_template\": math_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"physics\",\n",
    "        \"description\": \"answer physics puzzles\",\n",
    "        \"prompt_template\": physics_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"chemistry\",\n",
    "        \"description\": \"answer chemstry puzzles\",\n",
    "        \"prompt_template\": chemistry_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "destination_chains = {}\n",
    "\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]  \n",
    "    prompt_template = p_info[\"prompt_template\"] \n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=model, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(model, router_prompt)\n",
    "\n",
    "chain_multiprompt = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=LLMChain(llm=model, prompt=ChatPromptTemplate.from_template(chemistry_template)),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "my_question = {\"input\": \"explain redox reaction?\"}\n",
    "print(\"1. RouterRunnable output:\")\n",
    "pprint(chain_runnable.invoke(my_question))\n",
    "chain_runnable.get_graph().print_ascii()\n",
    "print('\\n')\n",
    "\n",
    "print(\"2. RunnableLambda output:\")\n",
    "pprint(chain_judge.invoke(my_question))\n",
    "pprint(chain_lambda.invoke(my_question))\n",
    "chain_lambda.get_graph().print_ascii()\n",
    "print('\\n')\n",
    "\n",
    "print(\"3. RunnableBranch output:\")\n",
    "pprint(chain_branch.invoke(my_question))\n",
    "chain_branch.get_graph().print_ascii()\n",
    "print('\\n')\n",
    "\n",
    "print(\"4. LLMRouterChain output:\")\n",
    "pprint(chain_multiprompt.invoke(my_question))\n",
    "chain_multiprompt.get_graph().print_ascii()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898f7697-e5fd-4cf0-9052-0c72d5d28ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
